{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "gpt = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo-1106\",\n",
    "    openai_api_key=\"7c3f9550b69c419aa0f1830e338ff562\",\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=\"https://chatbotopenaikeyswe.openai.azure.com/\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "path = \"C:/Users/Admin/Desktop/hsag chatbot/ragcsvdata\"\n",
    "csv_loader_kwargs={'autodetect_encoding': True}\n",
    "loader = DirectoryLoader(path, glob=\"**/*.csv\", loader_cls=CSVLoader, loader_kwargs=csv_loader_kwargs)\n",
    "db = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.thuega-energie-gmbh.de/privatkunden.html\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "db += html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embed = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_key=\"7c3f9550b69c419aa0f1830e338ff562\",\n",
    "    azure_endpoint=\"https://chatbotopenaikeyswe.openai.azure.com/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore = Chroma.from_documents(texts, embed, persist_directory=\"C:/chroma_db\")\n",
    "#vectorstore.persist()\n",
    "\n",
    "vectorstore2 = Chroma(persist_directory=\"C:/chroma_db\", embedding_function=embed)\n",
    "retriever = vectorstore2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant named \"hsag-chatbot\" for question-answering tasks \n",
    "related to the Energy industry and general conversation. \n",
    "\n",
    "If the question is related to the energy domain try to answer the question from the knowledge \n",
    "you have in your memory.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question \n",
    "or engage in small talk with the user in a friendly and informative way. \n",
    "If you don't know the answer to a factual question, \n",
    "just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "If the user asks funny questions or jokes, try to answer them \n",
    "using your knowledge or generate a humorous response. \n",
    "If they ask general knowledge about the world, try to answer \n",
    "those questions using your knowledge and also domain-specific knowledge \n",
    "about the energy industry. \n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm_model = gpt\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV modules, also known as photovoltaic modules, are the main building blocks of a solar power system. They are made up of multiple solar cells that convert sunlight into electricity, and are often used to generate renewable energy for homes and businesses. PV modules are an essential component of solar panel installations.\n",
      "[Document(page_content='Direkt zur Hauptnavigation springen  Direkt zum Inhalt springen\\n\\n  * Privatkunden \\n    * Strom\\n    * Erdgas\\n    * Wärme\\n    * Photovoltaik\\n    * E-Mobilität\\n    * Internet\\n    * Energieberatung\\n    * Förderprogramme\\n  * Geschäftskunden \\n    * Strom\\n      * Gewerbekunden\\n      * Industrie- und Großkunden\\n      * Wohnungswirtschaft\\n      * Kommunen\\n      * Energie- und Wasserversorger\\n    * Erdgas\\n      * Gewerbekunden\\n      * Industrie- und Großkunden\\n      * Wohnungswirtschaft\\n      * Kommunen\\n      * Energie- und Wasserversorgung\\n    * Contracting\\n    * Photovoltaik\\n    * E-Mobilität\\n    * Telekommunikation\\n    * Dienstleistungen\\n  * Über uns \\n    * Portrait\\n    * Nachhaltigkeit\\n    * Sponsoring\\n    * Auszeichnungen\\n    * Presse\\n  * Karriere \\n    * Stellenangebote\\n    * Ausbildung\\n    * Duales Studium\\n    * FAQ zu Ausbildung & Studium\\n  * Service \\n    * Kundenportale\\n    * Umzug melden\\n    * FAQ\\n    * Downloads\\n    * Kontakt\\n\\n  1.   2.   3.   4.   5. \\n\\n## **Photovoltaik-Aktion**\\n\\nBis zu 6 Module gratis sichern\\n\\nJetzt Schnellcheck machen\\n\\n## Rund um die Uhr ...  \\n**Umzug melden**\\n\\n  * An-, Um- und Abmeldung der Strom- und Gasversorgung\\n  * 5 Euro **Online-Bonus**\\n\\nJetzt Umzug melden\\n\\n  \\n5€ Bonus  \\nsichern!\\n\\n## Mein Ding ...  \\n**Klimaschonende Wärmeversorgung**\\n\\n  * Alle Informationen rund um das Gebäudeenergiengesetz\\n\\nJetzt mehr erfahren\\n\\n## Mein Ding ...  \\n**Strom aus regenerativen Energien**\\n\\n  * 100 % Ökostrom\\n  * Klima- und Umweltschutz\\n\\nWeitere Infos\\n\\n## Unser Ding ...  \\n**Nachhaltige Mobilität**\\n\\n  * Nutzung regenerativer Energieträger\\n  * Reduktion der CO2-Belastung\\n\\nJetzt mehr erfahren\\n\\n  1. __Home\\n  2. Privatkunden\\n\\n##### WICHTIGE INFOS\\n\\n×\\n\\n## Geänderte Öffnungszeiten bis Ende April\\n\\nMo 08:00 - 16:00 Uhr  \\nDi 08:00 - 18:00 Uhr  \\nMi **geschlossen**  \\nDo 08:00 - **12:30** Uhr  \\nFr 08:00 - 12:30 Uhr\\n\\nViele Anliegen können Sie über unsere Onlineservices selbst erledigen.  \\nDort erhalten Sie auch Informationen zu den Energiepreisbremsen.\\n\\nZum Servicebereich', metadata={'language': 'de-DE', 'source': 'https://www.thuega-energie-gmbh.de/privatkunden.html', 'title': 'Privatkunden | Thüga Energie GmbH'}), Document(page_content='id: changeEEGModule\\nSkillName: EEGSkill\\nactive: 1\\nskillId: EEG\\ndefaultEntityVirtualIntentName: PV-Anlagen (Modultausch)', metadata={'row': 132, 'source': 'C:\\\\Users\\\\Admin\\\\Desktop\\\\RAG pipeline\\\\Dataset\\\\CSVDATASET\\\\Intent_Overview.csv'}), Document(page_content='id: changeEEGModule\\nSkillName: EEGSkill\\nactive: 1\\nskillId: EEG\\ndefaultEntityVirtualIntentName: PV-Anlagen (Modultausch)', metadata={'row': 132, 'source': 'C:\\\\Users\\\\Admin\\\\Desktop\\\\RAG pipeline\\\\Dataset\\\\CSVDATASET\\\\Intent_Overview.csv'})]\n"
     ]
    }
   ],
   "source": [
    "question = \"What are PV modules \"\n",
    "\n",
    "result = rag_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)\n",
    "print(result[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    # Invoke the new chain with the updated structure\n",
    "    results = rag_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Extract the response from the result\n",
    "    response = result[\"response\"].content\n",
    "    \n",
    "    #Extract the context from the result\n",
    "    #context = result[\"context\"]\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    # Invoke the new chain with the updated structure\n",
    "    result = rag_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Extract the response from the result\n",
    "    response = result[\"response\"].content\n",
    "    \n",
    "    # Extract the context from the result\n",
    "    context = result[\"context\"]\n",
    "    \n",
    "    # Return the response in the desired structure\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"contexts\": [str(doc) for doc in context]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: PV modules, also known as photovoltaic modules, are solar panels that convert sunlight into electricity. Thüga Energie serves the regions of Hegau-Bodensee, Rhein-Pfalz, and Allgäu-Oberschwaben.\n",
      "Contexts: [\"page_content='Viele Anliegen können Sie über unsere Onlineservices selbst erledigen.  \\\\nDort erhalten Sie auch Informationen zu den Energiepreisbremsen.\\\\n\\\\nZum Servicebereich\\\\n\\\\nSchließen\\\\n\\\\n  * Strom Angebote\\\\n  * Erdgas Angebote\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __1  Person(en) im Haushalt\\\\n\\\\nWählen Sie die Personenzahl, oder tragen Sie Ihren Jahresverbrauch ein. (nur\\\\nZahlen zulässig)\\\\n\\\\nkWh im Jahr\\\\n\\\\nAktionscode\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __50 m2 Wohnfläche\\\\n\\\\nkWh im Jahr\\\\n\\\\nDieses Feld wird benötigt und darf nur Zahlen enthalten\\\\n\\\\nAktionscode\\\\n\\\\n#  Ihr Ansprechpartner rund um das Thema Energie\\\\n\\\\nWir, die Thüga Energie, sind Ihr **Energie\\\\xadversorgungs\\\\xadunternehmen** in den\\\\nRegionen Hegau-Bodensee, Rhein-Pfalz und Allgäu-Oberschwaben.\\\\n\\\\nMit unseren **innovativen Produkten** erschließen wir für Sie die **Zukunft\\\\nder Energie**.  \\\\nDazu gehören Erdgas, Strom und Wärme sowie Photovoltaik und Mobilität.\\\\n\\\\nAls regionaler Partner vor Ort stehen wir Ihnen mit einer **umfassenden\\\\nBeratung und Dienstleistungen** zu allen Energie-Themen zur Seite.\\\\n\\\\nWir freuen uns Sie beraten zu dürfen!  \\\\n\\\\n\\\\nKontakt\\\\n\\\\nKundenportale\\\\n\\\\nZählerstand melden, Abschläge ändern, Rechnung einsehen und Vieles mehr\\\\n\\\\nUmzugsservice\\\\n\\\\nGanz einfach Strom und Gas ummelden\\\\n\\\\nFAQ\\\\n\\\\nAntworten auf  häufig gestellte Fragen\\\\n\\\\nPreisblätter & Formulare\\\\n\\\\nAlle wichtigen Dokumente\\\\n\\\\n##  Aktuelles\\\\n\\\\n21.03.2024\\\\n\\\\n###  Minigärtner: Mit den Händen die Natur begreifen\\\\n\\\\nThüga Energie ist erneut Förderer der Regionalgruppe Singen\\\\n\\\\nWeiterlesen\\\\n\\\\n05.03.2024\\\\n\\\\n###  Energie selbst umwandeln, erleben und verstehen\\\\n\\\\nDie Thüga Energie fördert kreative Lernerfahrung von Kindern\\\\n\\\\nWeiterlesen\\\\n\\\\n19.02.2024\\\\n\\\\n###  Dank Warnwesten sicher zur Schule und zurück\\\\n\\\\nThüga Energie stattet Erstklässler der Schifferstadter Grundschulen Nord und\\\\nSüd aus\\\\n\\\\nWeiterlesen\\\\n\\\\n02.02.2024\\\\n\\\\n###  Thüga Energie übernimmt vertriebliche Betriebsführung der Gemeindewerke\\\\nRülzheim und des Gemeindewerks Hördt' metadata={'language': 'de-DE', 'source': 'https://www.thuega-energie-gmbh.de/privatkunden.html', 'title': 'Privatkunden | Thüga Energie GmbH'}\", \"page_content='Viele Anliegen können Sie über unsere Onlineservices selbst erledigen.  \\\\nDort erhalten Sie auch Informationen zu den Energiepreisbremsen.\\\\n\\\\nZum Servicebereich\\\\n\\\\nSchließen\\\\n\\\\n  * Strom Angebote\\\\n  * Erdgas Angebote\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __1  Person(en) im Haushalt\\\\n\\\\nWählen Sie die Personenzahl, oder tragen Sie Ihren Jahresverbrauch ein. (nur\\\\nZahlen zulässig)\\\\n\\\\nkWh im Jahr\\\\n\\\\nAktionscode\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __50 m2 Wohnfläche\\\\n\\\\nkWh im Jahr\\\\n\\\\nDieses Feld wird benötigt und darf nur Zahlen enthalten\\\\n\\\\nAktionscode\\\\n\\\\n#  Ihr Ansprechpartner rund um das Thema Energie\\\\n\\\\nWir, die Thüga Energie, sind Ihr **Energie\\\\xadversorgungs\\\\xadunternehmen** in den\\\\nRegionen Hegau-Bodensee, Rhein-Pfalz und Allgäu-Oberschwaben.\\\\n\\\\nMit unseren **innovativen Produkten** erschließen wir für Sie die **Zukunft\\\\nder Energie**.  \\\\nDazu gehören Erdgas, Strom und Wärme sowie Photovoltaik und Mobilität.\\\\n\\\\nAls regionaler Partner vor Ort stehen wir Ihnen mit einer **umfassenden\\\\nBeratung und Dienstleistungen** zu allen Energie-Themen zur Seite.\\\\n\\\\nWir freuen uns Sie beraten zu dürfen!  \\\\n\\\\n\\\\nKontakt\\\\n\\\\nKundenportale\\\\n\\\\nZählerstand melden, Abschläge ändern, Rechnung einsehen und Vieles mehr\\\\n\\\\nUmzugsservice\\\\n\\\\nGanz einfach Strom und Gas ummelden\\\\n\\\\nFAQ\\\\n\\\\nAntworten auf  häufig gestellte Fragen\\\\n\\\\nPreisblätter & Formulare\\\\n\\\\nAlle wichtigen Dokumente\\\\n\\\\n##  Aktuelles\\\\n\\\\n21.03.2024\\\\n\\\\n###  Minigärtner: Mit den Händen die Natur begreifen\\\\n\\\\nThüga Energie ist erneut Förderer der Regionalgruppe Singen\\\\n\\\\nWeiterlesen\\\\n\\\\n05.03.2024\\\\n\\\\n###  Energie selbst umwandeln, erleben und verstehen\\\\n\\\\nDie Thüga Energie fördert kreative Lernerfahrung von Kindern\\\\n\\\\nWeiterlesen\\\\n\\\\n19.02.2024\\\\n\\\\n###  Dank Warnwesten sicher zur Schule und zurück\\\\n\\\\nThüga Energie stattet Erstklässler der Schifferstadter Grundschulen Nord und\\\\nSüd aus\\\\n\\\\nWeiterlesen\\\\n\\\\n02.02.2024\\\\n\\\\n###  Thüga Energie übernimmt vertriebliche Betriebsführung der Gemeindewerke\\\\nRülzheim und des Gemeindewerks Hördt' metadata={'language': 'de-DE', 'source': 'https://www.thuega-energie-gmbh.de/privatkunden.html', 'title': 'Privatkunden | Thüga Energie GmbH'}\", \"page_content='Viele Anliegen können Sie über unsere Onlineservices selbst erledigen.  \\\\nDort erhalten Sie auch Informationen zu den Energiepreisbremsen.\\\\n\\\\nZum Servicebereich\\\\n\\\\nSchließen\\\\n\\\\n  * Strom Angebote\\\\n  * Erdgas Angebote\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __1  Person(en) im Haushalt\\\\n\\\\nWählen Sie die Personenzahl, oder tragen Sie Ihren Jahresverbrauch ein. (nur\\\\nZahlen zulässig)\\\\n\\\\nkWh im Jahr\\\\n\\\\nAktionscode\\\\n\\\\nLeider wird dieses Gebiet nicht beliefert\\\\n\\\\nPostleitzahl\\\\n\\\\n __50 m2 Wohnfläche\\\\n\\\\nkWh im Jahr\\\\n\\\\nDieses Feld wird benötigt und darf nur Zahlen enthalten\\\\n\\\\nAktionscode\\\\n\\\\n#  Ihr Ansprechpartner rund um das Thema Energie\\\\n\\\\nWir, die Thüga Energie, sind Ihr **Energie\\\\xadversorgungs\\\\xadunternehmen** in den\\\\nRegionen Hegau-Bodensee, Rhein-Pfalz und Allgäu-Oberschwaben.\\\\n\\\\nMit unseren **innovativen Produkten** erschließen wir für Sie die **Zukunft\\\\nder Energie**.  \\\\nDazu gehören Erdgas, Strom und Wärme sowie Photovoltaik und Mobilität.\\\\n\\\\nAls regionaler Partner vor Ort stehen wir Ihnen mit einer **umfassenden\\\\nBeratung und Dienstleistungen** zu allen Energie-Themen zur Seite.\\\\n\\\\nWir freuen uns Sie beraten zu dürfen!  \\\\n\\\\n\\\\nKontakt\\\\n\\\\nKundenportale\\\\n\\\\nZählerstand melden, Abschläge ändern, Rechnung einsehen und Vieles mehr\\\\n\\\\nUmzugsservice\\\\n\\\\nGanz einfach Strom und Gas ummelden\\\\n\\\\nFAQ\\\\n\\\\nAntworten auf  häufig gestellte Fragen\\\\n\\\\nPreisblätter & Formulare\\\\n\\\\nAlle wichtigen Dokumente\\\\n\\\\n##  Aktuelles\\\\n\\\\n21.03.2024\\\\n\\\\n###  Minigärtner: Mit den Händen die Natur begreifen\\\\n\\\\nThüga Energie ist erneut Förderer der Regionalgruppe Singen\\\\n\\\\nWeiterlesen\\\\n\\\\n05.03.2024\\\\n\\\\n###  Energie selbst umwandeln, erleben und verstehen\\\\n\\\\nDie Thüga Energie fördert kreative Lernerfahrung von Kindern\\\\n\\\\nWeiterlesen\\\\n\\\\n19.02.2024\\\\n\\\\n###  Dank Warnwesten sicher zur Schule und zurück\\\\n\\\\nThüga Energie stattet Erstklässler der Schifferstadter Grundschulen Nord und\\\\nSüd aus\\\\n\\\\nWeiterlesen\\\\n\\\\n02.02.2024\\\\n\\\\n###  Thüga Energie übernimmt vertriebliche Betriebsführung der Gemeindewerke\\\\nRülzheim und des Gemeindewerks Hördt' metadata={'language': 'de-DE', 'source': 'https://www.thuega-energie-gmbh.de/privatkunden.html', 'title': 'Privatkunden | Thüga Energie GmbH'}\"]\n"
     ]
    }
   ],
   "source": [
    "question = \"What are PV Modules? And to which regions does Thuga Energie serve?\"\n",
    "response = get_response(question)\n",
    "print(\"Answer:\", response[\"answer\"])\n",
    "print(\"Contexts:\", response[\"contexts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langsmith Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (0.1.12)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langsmith in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (0.1.27)\n",
      "Collecting langsmith\n",
      "  Downloading langsmith-0.1.63-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (3.9.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_core-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith) (3.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "   ---------------------------------------- 0.0/973.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 122.9/973.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 563.2/973.5 kB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  972.8/973.5 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 973.5/973.5 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.8/122.8 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  307.2/308.5 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 308.5/308.5 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.27\n",
      "    Uninstalling langsmith-0.1.27:\n",
      "      Successfully uninstalled langsmith-0.1.27\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.52\n",
      "    Uninstalling langchain-core-0.1.52:\n",
      "      Successfully uninstalled langchain-core-0.1.52\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.0.1\n",
      "    Uninstalling langchain-text-splitters-0.0.1:\n",
      "      Successfully uninstalled langchain-text-splitters-0.0.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.12\n",
      "    Uninstalling langchain-0.1.12:\n",
      "      Successfully uninstalled langchain-0.1.12\n",
      "Successfully installed langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script langsmith.exe is installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langchain-server.exe is installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.3 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.2.1 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the Gas contract number of the customer named Sebastian?\",\n",
    "    \"What is the meter number for meterId 1244 and what is its meter type?\",\n",
    "    \"What is the Gas contract number, IBAN, street, start of the contract, city,of the customer named Sebastian?\",\n",
    "    \"What is the purpose of the charging instruction in the EMobilitySkill and its connection to the charging payment aspect?\",\n",
    "    \"What services does Thuga Energie provide as a regional partner in the Hegau-Bodensee, Rhein-Pfalz, and Allgau-Oberschwaben regions?\",\n",
    "    \"What are the details of the meter with meterId 1244 in the Power section?\",\n",
    "    \"What options are there for making changes in our Kundenportal, and how can open questions be resolved with Kundenservice?\",\n",
    "    \"How do I delete historical meter readings and what metering systems automate reading collection and consumption data?\",\n",
    "    \"What are PV Modules? And to which regions does Thuga Energie serve?\",\n",
    "    \"Where can I find the meter number for my electricity or gas meter?\"\n",
    "    \n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"The gas contract number for the customer named Sebastian is ES00000062980\",\n",
    "    \"The meter number for meterId 1244 is ZEI2332DE2 and its meter type is ZTZ\",\n",
    "    \"The Gas contract number is ES00000062980, the IBAN is DE89370400440532013000, the street is Schmiedeweg, the start of the contract is 2022-03-14, and the city is Lauenburg\",\n",
    "    \"The purpose of the charging instruction in the EMobilitySkill is to facilitate the process of charging electric vehicles. Its connection to the charging payment aspect is to ensure that the costs of charging are properly managed and paid for.\",\n",
    "    \"Thuga Energie provides innovative products such as Erdgas, Strom, WÃƒÂ¤rme, Photovoltaik, and MobilitÃƒÂ¤t. They also offer comprehensive Beratung and Dienstleistungen to their customers.\",\n",
    "    \"The details of the meter with meterId 1244 in the Power section are as follows: meterNumber: ZEI2332DE2, validTo: 2023-05-04 23:59:59, readable: False, meterType: ZTZ, digitsBeforeDecimal: 5, digitsAfterDecimal: 0, registerId: 2, obis: 1-1:1.8.2\",\n",
    "    \"The Kundenportal offers the option to make changes online at any time, and open questions can be resolved with the Kundenservice through online communication. Additionally, human colleagues are available to assist with any queries. Rechnungen are received via email, and payments can be made through direct debit or bank transfer.\",\n",
    "    \"To delete historical meter readings, it is best to directly contact your metering point operator. Automated reading collection and consumption data are typically associated with smart metering systems, which transmit data to the smart meter gateway every 15 minutes. The data is then processed and transmitted to the respective metering point operator via the mobile network. For more detailed information, it is advisable to contact your network operator or metering point operator, whose contact details can be found on your most recent electricity or gas bill, as well as on your meter.\",\n",
    "    \"PV modules are also known as photovoltaic modules, they are solar panels that convert sunlight into electricity. ThÃ¼ga Energie serves the regions of Hegau-Bodensee, Rhein-Pfalz, and AllgÃ¤u-Oberschwaben as part of its energy supply services.\",\n",
    "    \"You can find the meter number for your electricity or gas meter on the front of the device, often near a barcode.\",\n",
    "]\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about EnergyIndustry.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "gpt4 = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4-128k\",\n",
    "    openai_api_key=\"7c3f9550b69c419aa0f1830e338ff562\",\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=\"https://chatbotopenaikeyswe.openai.azure.com/\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = get_response(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = get_response(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness: QA evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-qaevaluation-17534cf6' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=9d6c4637-3f7a-48d7-9b9f-a29f89d232cc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "eval_llm = gpt4\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm},\n",
    "                                         prepare_data=lambda run, example: {\n",
    "                                             \"prediction\": run.outputs[\"answer\"],\n",
    "                                             \"reference\": example.outputs[\"answer\"],\n",
    "                                             \"input\": example.inputs[\"question\"],\n",
    "                                         } )]\n",
    "\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators= qa_evaluator,\n",
    "    experiment_prefix=\"rag-qa-oai-qaevaluation\",\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COT_QA Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-cotqa-db11207f' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=9124ed0f-c34e-456d-b777-7d6f888e2c87\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:28,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "eval_llm = gpt4\n",
    "\n",
    "\n",
    "cot_qa_evaluator = [LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm},\n",
    "                                         prepare_data=lambda run, example: {\n",
    "                                             \"prediction\": run.outputs[\"answer\"],\n",
    "                                             \"reference\": example.outputs[\"answer\"],\n",
    "                                             \"input\": example.inputs[\"question\"],\n",
    "                                         } )]\n",
    "#qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators= cot_qa_evaluator,\n",
    "    experiment_prefix=\"rag-qa-oai-cotqa\",\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-contextual accuracy-507b7bfe' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=b8746b22-1ec3-466d-b273-6f345c4cc5e7\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "eval_llm = gpt4\n",
    "\n",
    "\n",
    "context_qa_evaluator = [LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm},\n",
    "                                         prepare_data=lambda run, example: {\n",
    "                                             \"prediction\": run.outputs[\"answer\"],\n",
    "                                             \"reference\": example.outputs[\"answer\"],\n",
    "                                             \"input\": example.inputs[\"question\"],\n",
    "                                         } )]\n",
    "#qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators= context_qa_evaluator,\n",
    "    experiment_prefix=\"rag-qa-oai-contextual accuracy\",\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt \n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = gpt4\n",
    "\n",
    "    # Structured prompt\n",
    "    \n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-144ff955' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/51858942-f9d0-4726-98ab-3b095d3d56d3/compare?selectedSessions=7b3631db-c9d6-4a59-a3bd-5c3c0fe2b816\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The method `ChatOpenAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n",
      "10it [00:19,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "eval_llm = gpt4\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? \n",
    "            A score of 0 means that the Assistant answer contains is not at all based upon / grounded in the\n",
    "            Ground Truth documentation. A score of 5 means that the Assistant score contains some information\n",
    "            (e.g., a hallucination) that is not captured in the  Ground Truth documentation. A score of 10 means \n",
    "            that the Assistant answer is fully based upon / grounded in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        \"normalize_by\": 10, \n",
    "        \n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-hallucination-9094677e' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=431e3641-0a93-410b-a0e4-c496e07cf70f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:11,  5.06s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6899ca98-af22-42dc-a825-b5919f28a4cc: ValueError(\"Invalid output: The Assistant's answer is not based on the Ground Truth information provided. The Ground Truth documentation contains specific log data and metadata related to a conversation ID and a meter reading record, but it does not provide instructions or information on how to delete historical meter readings or details about metering systems that automate reading collection and consumption data. The Assistant's response appears to be a generic explanation that is not grounded in the specifics of the provided Ground Truth.\\n\\nGiven that the response is not supported by the Ground Truth documentation, the rating would be as follows:\\n\\n[[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 567, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 562, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 148, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    return self.create_outputs(response)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 284, in create_outputs\n",
      "    self.output_key: self.output_parser.parse_result(generation),\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
      "    raise ValueError(\n",
      "ValueError: Invalid output: The Assistant's answer is not based on the Ground Truth information provided. The Ground Truth documentation contains specific log data and metadata related to a conversation ID and a meter reading record, but it does not provide instructions or information on how to delete historical meter readings or details about metering systems that automate reading collection and consumption data. The Assistant's response appears to be a generic explanation that is not grounded in the specifics of the provided Ground Truth.\n",
      "\n",
      "Given that the response is not supported by the Ground Truth documentation, the rating would be as follows:\n",
      "\n",
      "[[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "10it [00:19,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Relevance to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = gpt4\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "eval_llm = gpt4\n",
    "\n",
    "docs_relevance_evaluator = LangChainStringEvaluator(\n",
    "    \"score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"The Assistant's Answer is a set of documents retrieved from a vectorstore. The input is\n",
    "            a question used for retrieval. You will score whether the Assistant's answer (retrieved docs) are relevant to \n",
    "            the input question. A score of [[0]] means that the Assistant answer contains documents that are not at all\n",
    "            relevant to the input question. A score of[[5]] means that the Assistant answer contains some documents \n",
    "            are relevant to the input question. A score of [[10]] means that all of the Assistant answer documents are all\n",
    "            relevant to the input question\"\"\"\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-doc-relevance-8bd4b0bc' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=9b0e1559-c592-49b0-ad5d-8ff896241b5c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:10,  1.27s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2797623d-e3dc-4673-ae3c-3c2b844f2746: ValueError(\"Invalid output: The Assistant's answer appears to be a set of excerpts from some form of dataset logs and CSV data related to meter readings. The first two excerpts are from a system log with metadata about the conversation, likely related to meter reading queries but do not provide any information on how to delete historical meter readings or what systems automate reading collection. The third excerpt provides details of a specific meter reading from a CSV dataset, including meterId, meterNumber, and other technical details.\\n\\nNone of the provided excerpts directly answer the user's question about deleting historical meter readings or mention any metering systems that automate reading collection and consumption data. The response seems to be system-generated logs and data which are not intended to be informative answers to the question posed. Therefore, the documents retrieved are not relevant to the input question.\\n\\nRating: [[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 567, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 562, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 148, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    return self.create_outputs(response)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 284, in create_outputs\n",
      "    self.output_key: self.output_parser.parse_result(generation),\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
      "    raise ValueError(\n",
      "ValueError: Invalid output: The Assistant's answer appears to be a set of excerpts from some form of dataset logs and CSV data related to meter readings. The first two excerpts are from a system log with metadata about the conversation, likely related to meter reading queries but do not provide any information on how to delete historical meter readings or what systems automate reading collection. The third excerpt provides details of a specific meter reading from a CSV dataset, including meterId, meterNumber, and other technical details.\n",
      "\n",
      "None of the provided excerpts directly answer the user's question about deleting historical meter readings or mention any metering systems that automate reading collection and consumption data. The response seems to be system-generated logs and data which are not intended to be informative answers to the question posed. Therefore, the documents retrieved are not relevant to the input question.\n",
      "\n",
      "Rating: [[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "10it [00:23,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-doc-relevance\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-full-evaluation-7bdf657c' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=0c2c446b-d034-42ce-84c8-9ad7fe517cb3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:27,  3.76s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 60b1b64c-e485-4e43-9a35-befedf3716fd: ValueError(\"Invalid output: The Assistant's answer does not refer directly to the Ground Truth documentation provided, which does not include information on how to delete historical meter readings or details about metering systems that automate reading collection and consumption data. The response provided by the Assistant instead offers a general explanation of the process one might follow to delete historical meter readings and describes common technologies used in automated metering systems. While this might be useful information in a broader context, it does not adhere to the Ground Truth documents.\\n\\nAccuracy: The answer is not grounded in the provided Ground Truth documentation, as the documents do not contain information related to the deletion of historical meter readings or specific metering systems that automate the collection of readings and consumption data.\\nRating: [[0]]\\n\\nCompleteness: The answer does not address the specific Ground Truth content and instead provides generic information that does not reference the provided documentation.\\nRating: [[0]]\\n\\nCoherence: Despite not being grounded in the Ground Truth documentation, the Assistant's answer is well-structured, logically presented, and easy to understand.\\nRating: [[7]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 567, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 562, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 148, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    return self.create_outputs(response)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 284, in create_outputs\n",
      "    self.output_key: self.output_parser.parse_result(generation),\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
      "    raise ValueError(\n",
      "ValueError: Invalid output: The Assistant's answer does not refer directly to the Ground Truth documentation provided, which does not include information on how to delete historical meter readings or details about metering systems that automate reading collection and consumption data. The response provided by the Assistant instead offers a general explanation of the process one might follow to delete historical meter readings and describes common technologies used in automated metering systems. While this might be useful information in a broader context, it does not adhere to the Ground Truth documents.\n",
      "\n",
      "Accuracy: The answer is not grounded in the provided Ground Truth documentation, as the documents do not contain information related to the deletion of historical meter readings or specific metering systems that automate the collection of readings and consumption data.\n",
      "Rating: [[0]]\n",
      "\n",
      "Completeness: The answer does not address the specific Ground Truth content and instead provides generic information that does not reference the provided documentation.\n",
      "Rating: [[0]]\n",
      "\n",
      "Coherence: Despite not being grounded in the Ground Truth documentation, the Assistant's answer is well-structured, logically presented, and easy to understand.\n",
      "Rating: [[7]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "10it [00:45,  4.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "eval_llm = gpt4\n",
    "\n",
    "# Define the evaluation criteria for various aspects of the RAG model's output\n",
    "criteria = {\n",
    "    \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation?\n",
    "    A score of 0 means that the Assistant's answer is not at all based upon/grounded in the\n",
    "    Ground Truth documentation. A score of 10 means that the Assistant's answer is fully based upon/grounded in the Ground Truth documentation.\"\"\",\n",
    "    \n",
    "    \"completeness\": \"\"\"Does the Assistant's Answer cover all aspects of the question asked?\n",
    "    A score of 0 means that the Assistant's answer misses most of the key points required to fully address the question.\n",
    "    A score of 10 means that the Assistant's answer thoroughly covers all aspects of the question.\"\"\",\n",
    "    \n",
    "    \"coherence\": \"\"\"Is the Assistant's Answer logically coherent and easy to understand?\n",
    "    A score of 0 means that the Assistant's answer is disjointed and difficult to follow.\n",
    "    A score of 10 means that the Assistant's answer is logically structured and easy to follow.\"\"\"\n",
    "}\n",
    "\n",
    "# Create the evaluator\n",
    "rag_model_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": criteria,\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the dataset name\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "# Evaluate the RAG model using the defined evaluators\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[rag_model_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-full-evaluation\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'prompt-evaluation-eb641060' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=155b330b-7eef-4594-becb-356fcb61fb0c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:32,  3.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Prompt Template\n",
    "template = \"\"\"\n",
    "You are an assistant named \"hsag-chatbot\" for question-answering tasks \n",
    "related to the Energy industry and general conversation. \n",
    "\n",
    "If the question is related to the energy domain try to answer the question from the knowledge \n",
    "you have in your memory.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question \n",
    "or engage in small talk with the user in a friendly and informative way. \n",
    "If you don't know the answer to a factual question, \n",
    "just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "If the user asks funny questions or jokes, try to answer them \n",
    "using your knowledge or generate a humorous response. \n",
    "If they ask general knowledge about the world, try to answer \n",
    "those questions using your knowledge and also domain-specific knowledge \n",
    "about the energy industry. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Use the ChatPromptTemplate to create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 4: Define Evaluation Criteria\n",
    "criteria = {\n",
    "    \"clarity\": \"\"\"How clear and understandable is the prompt?\n",
    "    A score of 0 means the prompt is very unclear and confusing. A score of 10 means the prompt is very clear and easy to understand.\"\"\",\n",
    "    \n",
    "    \"specificity\": \"\"\"How specific is the prompt in guiding the model towards the desired response?\n",
    "    A score of 0 means the prompt is very vague. A score of 10 means the prompt is very specific.\"\"\",\n",
    "    \n",
    "    \"brevity\": \"\"\"Is the prompt concise and to the point without unnecessary information?\n",
    "    A score of 0 means the prompt is verbose. A score of 10 means the prompt is very concise.\"\"\",\n",
    "    \n",
    "    \"relevance\": \"\"\"How relevant is the prompt to the task at hand?\n",
    "    A score of 0 means the prompt is not relevant at all. A score of 10 means the prompt is highly relevant.\"\"\",\n",
    "    \n",
    "    \"effectiveness\": \"\"\"How effectively does the prompt elicit the desired response from the model?\n",
    "    A score of 0 means the prompt does not effectively guide the model. A score of 10 means the prompt very effectively guides the model.\"\"\"\n",
    "}\n",
    "\n",
    "prompt_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": criteria,\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[prompt_evaluator],\n",
    "    experiment_prefix=\"prompt-evaluation-oo1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'prompt-evaluation-oo1-ad221f99' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=918b6323-0728-4055-ba97-9c773fe61e31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:35,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Prompt Template\n",
    "template = \"\"\"\n",
    "You are an assistant named \"hsag-chatbot\" for question-answering tasks \n",
    "related to the Energy industry and general conversation. \n",
    "\n",
    "If the question is related to the energy domain try to answer the question from the knowledge \n",
    "you have in your memory.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question \n",
    "or engage in small talk with the user in a friendly and informative way. \n",
    "If you don't know the answer to a factual question, \n",
    "just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "If the user asks funny questions or jokes, try to answer them \n",
    "using your knowledge or generate a humorous response. \n",
    "If they ask general knowledge about the world, try to answer \n",
    "those questions using your knowledge and also domain-specific knowledge \n",
    "about the energy industry. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Use the ChatPromptTemplate to create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 4: Define Evaluation Criteria\n",
    "criteria = {\n",
    "    \"clarity\": \"\"\"How clear and understandable is the prompt?\n",
    "    A score of 0 means the prompt is very unclear and confusing. A score of 10 means the prompt is very clear and easy to understand.\"\"\",\n",
    "    \n",
    "    \"specificity\": \"\"\"How specific is the prompt in guiding the model towards the desired response?\n",
    "    A score of 0 means the prompt is very vague. A score of 10 means the prompt is very specific.\"\"\",\n",
    "    \n",
    "    \"brevity\": \"\"\"Is the prompt concise and to the point without unnecessary information?\n",
    "    A score of 0 means the prompt is verbose. A score of 10 means the prompt is very concise.\"\"\",\n",
    "    \n",
    "    \"relevance\": \"\"\"How relevant is the prompt to the task at hand?\n",
    "    A score of 0 means the prompt is not relevant at all. A score of 10 means the prompt is highly relevant.\"\"\",\n",
    "    \n",
    "    \"effectiveness\": \"\"\"How effectively does the prompt elicit the desired response from the model?\n",
    "    A score of 0 means the prompt does not effectively guide the model. A score of 10 means the prompt very effectively guides the model.\"\"\"\n",
    "}\n",
    "\n",
    "prompt_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": criteria,\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": prompt.format(context=\"\", question=run.outputs[\"answer\"]),\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[prompt_evaluator],\n",
    "    experiment_prefix=\"prompt-evaluation-oo1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'prompt-evaluation-oo1-c249960d' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=7d3a4392-43c8-4775-8a87-780f052d658c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:19,  4.85s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run a24b9b91-5c17-418f-8eaa-9c37c8092425: ValueError('Invalid output: The provided response is not an actual answer to the user\\'s question but rather an instruction to the AI on how to behave as an assistant. The user asks about options for making changes in the Kundenportal and resolving open questions with Kundenservice, while the response gives a directive to the AI named \"hsag-chatbot\" on how to answer questions.\\n\\nExplanation:\\n- Clarity: The prompt template provided to the AI is clear in its instructions on what the AI should do when answering questions. However, it does not answer the user\\'s question, so it cannot be rated highly in clarity for the user\\'s perspective.\\n- Specificity: The prompt is specific in guiding the AI on its general behavior when interacting with users, but it does not provide specific guidance for the user\\'s question, making it off-target.\\n- Brevity: The prompt is not brief in relation to the user\\'s question, as it includes instructions irrelevant to the actual answer required.\\n- Relevance: The prompt is not relevant to the user\\'s question about the Kundenportal and Kundenservice, as it doesn\\'t provide any information about these services.\\n- Effectiveness: The prompt is not effective in eliciting the desired response from the model concerning the user\\'s question, as it fails to provide an answer to the query about the Kundenportal and Kundenservice.\\n\\nRating: [[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 567, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 562, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 148, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    return self.create_outputs(response)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 284, in create_outputs\n",
      "    self.output_key: self.output_parser.parse_result(generation),\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
      "    raise ValueError(\n",
      "ValueError: Invalid output: The provided response is not an actual answer to the user's question but rather an instruction to the AI on how to behave as an assistant. The user asks about options for making changes in the Kundenportal and resolving open questions with Kundenservice, while the response gives a directive to the AI named \"hsag-chatbot\" on how to answer questions.\n",
      "\n",
      "Explanation:\n",
      "- Clarity: The prompt template provided to the AI is clear in its instructions on what the AI should do when answering questions. However, it does not answer the user's question, so it cannot be rated highly in clarity for the user's perspective.\n",
      "- Specificity: The prompt is specific in guiding the AI on its general behavior when interacting with users, but it does not provide specific guidance for the user's question, making it off-target.\n",
      "- Brevity: The prompt is not brief in relation to the user's question, as it includes instructions irrelevant to the actual answer required.\n",
      "- Relevance: The prompt is not relevant to the user's question about the Kundenportal and Kundenservice, as it doesn't provide any information about these services.\n",
      "- Effectiveness: The prompt is not effective in eliciting the desired response from the model concerning the user's question, as it fails to provide an answer to the query about the Kundenportal and Kundenservice.\n",
      "\n",
      "Rating: [[0]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "10it [00:40,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Prompt Template\n",
    "template = \"\"\"\n",
    "You are an assistant named \"hsag-chatbot\" for question-answering tasks \n",
    "related to the Energy industry and general conversation. \n",
    "\n",
    "If the question is related to the energy domain try to answer the question from the knowledge \n",
    "you have in your memory.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question \n",
    "or engage in small talk with the user in a friendly and informative way. \n",
    "If you don't know the answer to a factual question, \n",
    "just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "If the user asks funny questions or jokes, try to answer them \n",
    "using your knowledge or generate a humorous response. \n",
    "If they ask general knowledge about the world, try to answer \n",
    "those questions using your knowledge and also domain-specific knowledge \n",
    "about the energy industry. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Use the ChatPromptTemplate to create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 4: Define Evaluation Criteria\n",
    "criteria = {\n",
    "    \"clarity\": \"\"\"How clear and understandable is the prompt template?\n",
    "    A score of 0 means the prompt is very unclear and confusing. A score of 10 means the prompt is very clear and easy to understand.\"\"\",\n",
    "    \n",
    "    \"specificity\": \"\"\"How specific is the prompt template in guiding the model towards the desired response?\n",
    "    A score of 0 means the prompt is very vague. A score of 10 means the prompt is very specific.\"\"\",\n",
    "    \n",
    "    \"brevity\": \"\"\"Is the prompt template concise and to the point without unnecessary information?\n",
    "    A score of 0 means the prompt is verbose. A score of 10 means the prompt is very concise.\"\"\",\n",
    "    \n",
    "    \"relevance\": \"\"\"How relevant is the prompt template to the task at hand?\n",
    "    A score of 0 means the prompt is not relevant at all. A score of 10 means the prompt is highly relevant.\"\"\",\n",
    "    \n",
    "    \"effectiveness\": \"\"\"How effectively does the prompt template elicit the desired response from the model?\n",
    "    A score of 0 means the prompt does not effectively guide the model. A score of 10 means the prompt very effectively guides the model.\"\"\"\n",
    "}\n",
    "\n",
    "prompt_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": criteria,\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": prompt.format(\n",
    "            context=run.outputs.get(\"contexts\", \"\"),  # Use contexts if available, otherwise set to empty string\n",
    "            question=run.outputs[\"answer\"]\n",
    "        ),\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[prompt_evaluator],\n",
    "    experiment_prefix=\"prompt-evaluation-oo1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'prompt-evaluation-oo1-ba41f292' at:\n",
      "https://smith.langchain.com/o/193e2b09-24bf-506a-879c-4ee7e4127ca4/datasets/a3d891b4-fc65-4205-a80f-6929ca34e386/compare?selectedSessions=cd06b9c6-6e7d-4ba0-bec5-b9680b1384dd\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:52,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Prompt Template\n",
    "template = \"\"\"\n",
    "You are an assistant named \"hsag-chatbot\" for question-answering tasks \n",
    "related to the Energy industry and general conversation. \n",
    "\n",
    "If the question is related to the energy domain try to answer the question from the knowledge \n",
    "you have in your memory.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question \n",
    "or engage in small talk with the user in a friendly and informative way. \n",
    "If you don't know the answer to a factual question, \n",
    "just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "If the user asks funny questions or jokes, try to answer them \n",
    "using your knowledge or generate a humorous response. \n",
    "If they ask general knowledge about the world, try to answer \n",
    "those questions using your knowledge and also domain-specific knowledge \n",
    "about the energy industry. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "eval_llm = gpt4\n",
    "# Use the ChatPromptTemplate to create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 4: Define Evaluation Criteria\n",
    "criteria = {\n",
    "    \"clarity\": \"\"\"How clear and understandable is the prompt template?\n",
    "    A score of 0 means the prompt is very unclear and confusing. A score of 10 means the prompt is very clear and easy to understand.\"\"\",\n",
    "    \n",
    "    \"specificity\": \"\"\"How specific is the prompt template in guiding the model towards the desired response?\n",
    "    A score of 0 means the prompt is very vague. A score of 10 means the prompt is very specific.\"\"\",\n",
    "    \n",
    "    \"brevity\": \"\"\"Is the prompt template concise and to the point without unnecessary information?\n",
    "    A score of 0 means the prompt is verbose. A score of 10 means the prompt is very concise.\"\"\",\n",
    "    \n",
    "    \"relevance\": \"\"\"How relevant is the prompt template to the task at hand?\n",
    "    A score of 0 means the prompt is not relevant at all. A score of 10 means the prompt is highly relevant.\"\"\",\n",
    "    \n",
    "    \"effectiveness\": \"\"\"How effectively does the prompt template elicit the desired response from the model?\n",
    "    A score of 0 means the prompt does not effectively guide the model. A score of 10 means the prompt very effectively guides the model.\"\"\"\n",
    "}\n",
    "\n",
    "prompt_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"criteria\": criteria,\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": prompt.format(\n",
    "            context=run.outputs.get(\"contexts\", \"\"),  # Use contexts if available, otherwise set to empty string\n",
    "            answer=run.outputs[\"answer\"],\n",
    "            question=example.inputs[\"question\"]\n",
    "        ),\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = \"RAGEvaluation_testset1\"\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[prompt_evaluator],\n",
    "    experiment_prefix=\"prompt-evaluation-oo1\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
